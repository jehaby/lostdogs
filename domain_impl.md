this is codex-friendly **implementation plan** that yields **pure (side-effect-free)** code in two files: `domain.go` and `domain_test.go`. It sticks to **regexp + simple heuristics** only.

---

# domain.go ‚Äî Spec

## Package

`package vkdom`

## Data types

```go
// Canonical post result (pure data, no side effects)
type Post struct {
    ID            int            // not parsed from text; tests can set it
    Raw           string         // original text
    Type          PostType       // classified type
    Animal        AnimalType     // cat/dog/other/unknown
    Breed         string         // extracted breed or ""
    Sex           SexType        // m/f/unknown
    Age           string         // free text (e.g., "2,5 –º–µ—Å", "6-7 –º–µ—Å—è—Ü–µ–≤")
    Name          string         // pet name if confidently extracted (e.g., "–ú–∞—Å—è")
    Location      string         // simple heuristic span (street+number/city/area)
    When          string         // free text date/time if confidently found (e.g., "26.08.2025 22:00")
    Phones        []string       // normalized to +7XXXXXXXXXX
    ContactNames  []string       // human names near phones or vk mentions
    VKAccounts    []string       // vk.com links or [id...|Name] mentions (original form)
    Extras        Extras         // boolean flags
    StatusDetails string         // short trimmed summary of key descriptors
}

// Controlled enums (stringer not necessary)
type PostType string
const (
    TypeUnknown     PostType = "unknown"
    TypeLost        PostType = "lost"
    TypeFound       PostType = "found"
    TypeSighting    PostType = "sighting"
    TypeAdoption    PostType = "adoption"
    TypeFundraising PostType = "fundraising"
    TypeNews        PostType = "news"
    TypeLink        PostType = "link"
    TypeEmpty       PostType = "empty"
)

type AnimalType string
const (
    AnimalUnknown AnimalType = "unknown"
    AnimalCat     AnimalType = "cat"
    AnimalDog     AnimalType = "dog"
    AnimalOther   AnimalType = "other"
)

type SexType string
const (
    SexUnknown SexType = "unknown"
    SexM       SexType = "m"
    SexF       SexType = "f"
)

type Extras struct {
    Sterilized bool
    Vaccinated bool
    Chipped    bool
    LitterOK   bool
}
```

## Public API

```go
// Parse returns a fully-populated Post from raw text.
// Pure function: no I/O, no globals, deterministic.
func Parse(id int, raw string) Post
```

## Helpers (unexported; pure)

Implement these as small, testable pieces used by `Parse`:

```go
func normalizeSpace(s string) string
func detectType(s string) PostType
func extractPhones(s string) []string          // normalized to +7...
func extractVKAccounts(s string) []string
func extractContactNamesAroundPhones(s string, phones []string) []string
func detectAnimal(s string) AnimalType
func extractBreed(s string) string
func detectSex(s string) SexType
func extractAge(s string) string
func extractWhen(s string) string
func extractLocationHeuristic(s string) string
func extractStatusDetails(s string) string
func extractPetName(s string) string
```

## Regex/heuristics (exact patterns to use)

* **Lost**:

  * `\b(–ø—Ä–æ–ø–∞–ª[–∞–∏]?|–ø–æ—Ç–µ—Ä—è–ª[–∞—Å—å—Å—è]?|—É–±–µ–∂–∞–ª[–∞–∏]?|—Å–±–µ–∂–∞–ª[–∞–∏]?)\b`

* **Found**:

  * `\b(–Ω–∞–π–¥–µ–Ω[–∞–æ—ã]?|–Ω–∞—à–ª[–∞–∏]|–ø–æ–¥–æ–±—Ä–∞–ª[–∞–∏])\b`

* **Sighting**:

  * `\b(–∑–∞–º–µ—á–µ–Ω[–∞–æ]?|–≤–∏–¥–µ–ª[–∞–∏]?|–±–µ–≥–∞–µ—Ç|–ø–æ—è–≤–∏–ª[–∞—Å—å—Å—è])\b`

* **Adoption**:

  * `\b(–∏—â–µ—Ç\s+–¥–æ–º|–≤\s+–¥–æ–±—Ä—ã–µ\s+—Ä—É–∫–∏|–æ—Ç–¥–∞[—ë–º–º]|–ø—Ä–∏—Å—Ç—Ä–∞–∏–≤[–∞–µ])\b`
  * plus ‚Äúcare‚Äù markers: `—Å—Ç–µ—Ä–∏–ª–∏–∑|–∫–∞—Å—Ç—Ä–∏—Ä|–≤–∞–∫—Ü|–ø—Ä–∏–≤–∏—Ç|—á–∏–ø–∏—Ä|–ª–æ—Ç–æ–∫`

* **Fundraising**:

  * `\b(—Å–±–æ—Ä|–æ–ø–ª–∞—Ç–∏—Ç—å|–ø–µ—Ä–µ–≤–æ–¥|–ø–µ—Ä–µ–¥–µ—Ä–∂–∫|–∫–∞—Ä—Ç–∞)\b`

* **Link/Empty**:

  * if trimmed text is empty ‚Üí `TypeEmpty`
  * if contains only URLs/ids and ‚â§20 non-URL chars ‚Üí `TypeLink`

* **Tie-break rule**: If both lost/found matched, prefer the *more specific* sentence fragment:

  * If any `–Ω–∞–π–¥–µ–Ω.*(–∫–æ—Ç|—Å–æ–±–∞–∫|–ø—ë—Å|–ø–µ—Å|–∫–æ–±–µ–ª|—â–µ–Ω|–∂–∏–≤–æ—Ç)` ‚Üí Found
  * Else if any lost words present ‚Üí Lost

* **Phones** (Russian, normalize):

  * Raw match:

    ```
    (?:(?:\+7|8)\s*\(?\d{3}\)?[\s-]?\d{3}[\s-]?\d{2}[\s-]?\d{2}|\b9\d{2}[\s-]?\d{3}[\s-]?\d{2}[\s-]?\d{2}\b)
    ```
  * Normalization:

    * keep only digits
    * if starts with "8" and length 11 ‚Üí replace first with "+7"
    * if starts with "7" and length 11 ‚Üí prefix "+" ‚Üí "+7..."
    * if starts with "9" and length 10/11 ‚Üí prefix "+7"
    * final expected `+7XXXXXXXXXX` (11 digits total after +7)
    * dedupe while preserving first-seen order

* **Animal**:

  * Cat if matches: `–∫–æ—à–∫|–∫–æ—Ç(?!–µ–π–∫–∞)|–∫–æ—Ç[–µ—ë]–Ω|–∫–∏—Å[–∞–æ]–Ω—å–∫|–±–µ–Ω–≥–∞–ª—å—Å–∫`
  * Dog if matches: `—Å–æ–±–∞–∫|–ø—Å|–ø[–µ—ë]—Å\b|–ø[–µ—ë]—Å–∏–∫|–∫–æ–±–µ–ª|—â–µ–Ω`
  * Else unknown

* **Breed**: grab known fragments near animal words (case-insensitive):

  * `–±–µ–Ω–≥–∞–ª—å—Å–∫[–∞—è–∏–π–æ–µ]`, `–π–æ—Ä–∫(—à–∏—Ä(—Å–∫(–∏–π|–∞—è))?)?`, `–ª–∞–±—Ä–∞–¥–æ—Ä`, `–æ–≤—á–∞—Ä–∫`, `—Ö–∞—Å–∫`, `—Ç–∞–∫—Å`, `—Å–ø–∞–Ω–∏–µ–ª`, `–º–µ—Ç–∏—Å`
  * Return the matched token (title-cased)

* **Sex**:

  * Male: `\b–∫–æ–±–µ–ª(—ë|–µ)–∫|–º–∞–ª—å—á–∏–∫\b`
  * Female: `\b–¥–µ–≤–æ—á–∫|—Å—É–∫–∞\b`

* **Age** (loose):

  * Short capture: `(\d+[,.]?\d*|\d+\s*-\s*\d+)\s*(–º–µ—Å|–º–µ—Å—è—Ü|–º–µ—Å—è—Ü–µ–≤|–≥–æ–¥|–≥–æ–¥–∞|–ª–µ—Ç)`
  * Return the first concise match (normalize commas to dots)

* **When** (date/time):

  * Date: `\b\d{1,2}[.]\d{1,2}[.]\d{2,4}\b`
  * Time near date: `(?:–≤\s*)?\b\d{1,2}[:.]\d{2}\b`
  * If both present near each other, join `"DD.MM.YYYY HH:MM"`, else just date

* **Location heuristic**:

  * Find spans with tokens like:
    `—É–ª–∏—Ü|—à–æ—Å—Å|–ø—Ä–æ—Å–ø|–ø–µ—Ä(–µ)?—É–ª|–ø–ª–æ—â–∞–¥|–±—É–ª—å–≤–∞—Ä|—Ä–∞–π–æ–Ω|—Å–Ω—Ç|–≥–æ—Ä–æ–¥|–¥–µ—Ä–µ–≤–Ω|–ø–æ—Å(–µ)?–ª–æ–∫|–ò–∂–µ–≤—Å–∫|–ó–∞–∫–∏—Ä–æ–≤–∞|–ü–µ—Ä–≤–æ–º–∞–π—Å–∫|–õ—é–∫—à—É–¥—å—è|–®–∞–±–µ—Ä–¥–∏–Ω–æ|–í–æ—Ç–∫–∏–Ω—Å–∫`
  * Choose the shortest comma-delimited segment that contains one trigger and (optionally) a number; trim.

* **VK accounts**:

  * `vk\.com/\S+`
  * `\[(id\d+)\|([^\]]+)\]` (keep the whole `[id...|Name]`)

* **Contact names**:

  * Within ¬±25 chars of a phone, capture a single capitalized word in Cyrillic (`\b[–ê-–Ø–Å][–∞-—è—ë]{2,}\b`), exclude common nouns.
  * Add also from VK mention `[id..|–ò–º—è –§–∞–º–∏–ª–∏—è]` ‚Üí split and take first word.
  * Dedupe.

* **StatusDetails**:

  * Heuristic: collect up to \~140 chars of adjectives/traits from keywords:
    `—Ä—ã–∂|–±–µ–ª–æ—Å–Ω–µ–∂–Ω|–ø—É–≥–ª–∏–≤|–ª–∞—Å–∫–æ–≤|–∏–≥—Ä–∏–≤|–¥–æ–º–∞—à–Ω|–±–µ–∑ –æ—à–µ–π–Ω|–∫–∞—Å—Ç—Ä–∏—Ä|—Å—Ç–µ—Ä–∏–ª–∏–∑|–≤–∞–∫—Ü–∏–Ω|—á–∏–ø–∏—Ä|–ª–æ—Ç–æ–∫`
  * Join unique snippets with `, `

* **Pet name** (weak heuristic):

  * Pattern: `–∫–ª–∏—á–∫–∞\s+([A-Z–ê-–Ø–Å][\p{L}\-]{2,})` OR `–∫–æ—à–∫[–∞–∏]\s+([A-Z–ê-–Ø–Å][\p{L}\-]{2,})` within 20 chars of ‚Äú–∫–ª–∏—á–∫–∞‚Äù or ‚Äú–∑–æ–≤—É—Ç‚Äù.
  * If multiple, take the first.

## Parse flow

1. `s := normalizeSpace(raw)`
2. If empty ‚Üí `TypeEmpty`
3. Detect `Type` (apply tie-break).
4. Extract phones ‚Üí normalize & dedupe.
5. Extract VK accounts.
6. Detect Animal, Breed, Sex, Age.
7. Extract When, Location.
8. Extract Contact Names (using phones + VK mentions).
9. Build `StatusDetails` and `Name`.
10. Return `Post`.

---

# domain\_test.go ‚Äî Tests Plan

`package vkdom`

Use **table-driven tests**. No external deps. Each case provides `id`, `text`, and expected fields minimally necessary to validate the logic. Keep tests deterministic and small.

## Sample cases (from your real messages)

1. **Lost cat with full address + phone** (id 11 or 20)

```go
text := "–ü—Ä–æ–ø–∞–ª–∞ –∫–æ—à–∫–∞! ... –ü—É—à–∫–∏–Ω—Å–∫–∞—è —É–ª–∏—Ü–∞, 283, –ò–∂–µ–≤—Å–∫ ... –¢–µ–ª–µ—Ñ–æ–Ω: 79120281683 ... –ò–º—è —Ö–æ–∑—è–∏–Ω–∞: –Æ—Ä–∏–π ... –∫–æ—à–∫–∞ –ú–∞—Å—è"
Expect:
Type=TypeLost
Animal=AnimalCat
Phones=["+79120281683"]
Location contains "–ü—É—à–∫–∏–Ω—Å–∫–∞—è —É–ª–∏—Ü–∞, 283"
ContactNames includes "–Æ—Ä–∏–π"
Name == "–ú–∞—Å—è" (optional if ‚Äú–∫–ª–∏—á–∫–∞‚Äù absent; acceptable to not match ‚Äî keep test tolerant)
```

2. **Sighting dog, no phone** (id 22)

```go
"–ë–µ–≥–∞–µ—Ç –Ω–∞ –ó–∞–∫–∏—Ä–æ–≤–∞ –∏ –ü–µ—Ä–≤–æ–º–∞–π—Å–∫–æ–π –∫–æ–±–µ–ª–µ–∫ ... –ø–æ—Ö–æ–∂ –Ω–∞ –ô–æ—Ä–∫–∞ ... –ø–æ–π–º–∞—Ç—å –Ω–µ —Å–º–æ–≥–ª–∏"
Type=TypeSighting
Animal=AnimalDog
Breed contains "–π–æ—Ä–∫"
Location contains "–ó–∞–∫–∏—Ä–æ–≤–∞" and/or "–ü–µ—Ä–≤–æ–º–∞–π—Å–∫–æ–π"
len(Phones)=0
```

3. **Found cat with phone & name** (id 46)

```go
"–ù–∞–π–¥–µ–Ω –∫–æ—Ç. –†–∞–π–æ–Ω –ó–∞—Ä–µ—á–Ω–æ–µ —à–æ—Å—Å–µ 49 ... 89127500184 –ê–ª–µ–∫—Å–∞–Ω–¥—Ä"
Type=TypeFound
Animal=AnimalCat
Phones=["+79127500184"]
ContactNames contains "–ê–ª–µ–∫—Å–∞–Ω–¥—Ä"
Location contains "–ó–∞—Ä–µ—á–Ω–æ–µ —à–æ—Å—Å–µ 49"
```

4. **Adoption, sterilized, vaccinated** (id 18)

```go
"–∫—Ä–∞—Å–∏–≤–∞—è –∫–æ—à–µ—á–∫–∞ ... —Å—Ç–µ—Ä–∏–ª–∏–∑–æ–≤–∞–Ω–∞, –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞ ... –õ–æ—Ç–æ–∫ –Ω–∞ –æ—Ç–ª–∏—á–Ω–æ ... 8-912-762-92-39 –û–ª—å–≥–∞"
Type=TypeAdoption
Animal=AnimalCat
Extras.Sterilized=true
Extras.LitterOK=true
Phones has "+79127629239"
ContactNames contains "–û–ª—å–≥–∞"
```

5. **Fundraising** (id 14)

```go
"–ø–æ–º–æ—á—å –æ–ø–ª–∞—Ç–∏—Ç—å –ø–µ—Ä–µ–¥–µ—Ä–∂–∫—É ... –°—É–º–º–∞ –∫ —Å–±–æ—Ä—É ... üìû 8912 4586329 –ê–Ω–Ω–∞"
Type=TypeFundraising
Phones ["+79124586329"]
ContactNames contains "–ê–Ω–Ω–∞"
```

6. **Lost with explicit date/time** (id 36)

```go
"–ü–æ—Ç–µ—Ä—è–ª–∞—Å—å –∫–æ—à–∫–∞ ... –í–æ—Ç–∫–∏–Ω—Å–∫–æ–µ —à–æ—Å—Å–µ 39 26.08.2025 –ø—Ä–∏–º–µ—Ä–Ω–æ –≤ 22—á ... 89120216801"
Type=TypeLost
When contains "26.08.2025"   // time normalization tolerant ("22" or "22:00")
Location contains "–í–æ—Ç–∫–∏–Ω—Å–∫–æ–µ —à–æ—Å—Å–µ 39"
Phones ["+79120216801"]
```

7. **Adoption (kitten 2 months)** (id 27 or 40/41/42)

```go
"–ú–∞–ª—ã—à—É –æ–∫–æ–ª–æ 2—Ö –º–µ—Å—è—Ü–µ–≤ ... –ª–æ—Ç–∫–æ–º ... –ø–∏—à–∏—Ç–µ –Æ–ª–∏–∏ 89501684430"
Type=TypeAdoption
Age contains "2"
Phones ["+789501684430" ‚Üí "+789..." is invalid; expect "+7" normalization from "8950..." ‚Üí "+789501684430" WRONG]
Correct expectation: "+7 950 168-44-30" ‚Üí "+79501684430"
```

8. **Link-only** (id 21 or 45 or 49)

```go
"https://vk.com/wall107929440_36"
Type=TypeLink
```

9. **Empty** (id 12 or 24 or 30)

```go
""
Type=TypeEmpty
```

10. **Lost (long story across areas)** (id 32)

```go
"... –º—ã –ø–æ—Ç–µ—Ä—è–ª–∏ –∫–æ—Ç–∞ ... –õ—é–∫—à—É–¥—å—è –∏ –®–∞–±–µ—Ä–¥–∏–Ω–æ ... 89224052612- –¢–∞—Ç—å—è–Ω–∞"
Type=TypeLost
Phones ["+789224052612" ‚Üí normalize to "+789..."?] // Correct: Russian: 8 922 405 26 12 ‚Üí "+79224052612"
ContactNames includes "–¢–∞—Ç—å—è–Ω–∞"
Location contains "–õ—é–∫—à—É–¥—å—è" or "–®–∞–±–µ—Ä–¥–∏–Ω–æ"
```

11. **Adoption extras** (id 29)

```go
"—Å—Ç–µ—Ä–∏–ª–∏–∑–æ–≤–∞–Ω–∞, –≤–∞–∫—Ü–∏–Ω–∏—Ä–æ–≤–∞–Ω–∞. –õ–æ—Ç–æ–∫ –Ω–∞ –æ—Ç–ª–∏—á–Ω–æ"
Type=TypeAdoption
Extras.Sterilized=true
Extras.Vaccinated=true
Extras.LitterOK=true
```

12. **Duplicate content check (id 19 vs 44)**
    Both are similar adoption/rescue ads; not deduped here (no persistence). Just ensure consistent classification:

```go
Type=TypeAdoption
Animal=AnimalCat
Phones contain "+79068168418"
```

> Note: in tests 7 and 10 I highlighted the typical *normalization gotchas*. Put the **correct** expectations in code; comments can mention the original messy forms. (See expected arrays below.)

## Test structure

* `TestParse_Table` with an array of `{name, id, text, want}`.
* `want` can be a minimal struct or per-field checks to keep tests robust:

  * Always check `Type`
  * Check `Phones` (exact normalized slice)
  * For strings like `Location`, assert `strings.Contains(got.Location, "...")`
  * For booleans, direct equality
  * For `Animal`, `Sex`, `Extras`, direct equality

### Example expected values (ready to paste)

Use exactly these in the test table:

```go
// Case 1 (id=11)
want.Type = TypeLost
want.Animal = AnimalCat
want.Phones = []string{"+79120281683"}
want.LocationContains = []string{"–ü—É—à–∫–∏–Ω—Å–∫–∞—è —É–ª–∏—Ü–∞", "283"}
want.ContactNamesHas = []string{"–Æ—Ä–∏–π"}

// Case 2 (id=22)
want.Type = TypeSighting
want.Animal = AnimalDog
want.BreedContains = "–π–æ—Ä–∫"
want.LocationContains = []string{"–ó–∞–∫–∏—Ä–æ–≤–∞", "–ü–µ—Ä–≤–æ–º–∞–π—Å–∫"}
want.Phones = nil

// Case 3 (id=46)
want.Type = TypeFound
want.Animal = AnimalCat
want.Phones = []string{"+79127500184"}
want.ContactNamesHas = []string{"–ê–ª–µ–∫—Å–∞–Ω–¥—Ä"}
want.LocationContains = []string{"–ó–∞—Ä–µ—á–Ω–æ–µ —à–æ—Å—Å–µ", "49"}

// Case 4 (id=18)
want.Type = TypeAdoption
want.Animal = AnimalCat
want.ExtrasSterilized = true
want.ExtrasLitterOK = true
want.Phones = []string{"+79127629239"}
want.ContactNamesHas = []string{"–û–ª—å–≥–∞"}

// Case 5 (id=14)
want.Type = TypeFundraising
want.Phones = []string{"+79124586329"}
want.ContactNamesHas = []string{"–ê–Ω–Ω–∞"}

// Case 6 (id=36)
want.Type = TypeLost
want.WhenContains = []string{"26.08.2025"}
want.LocationContains = []string{"–í–æ—Ç–∫–∏–Ω—Å–∫–æ–µ —à–æ—Å—Å–µ", "39"}
want.Phones = []string{"+79120216801"}

// Case 7 (id=27)
want.Type = TypeAdoption
want.AgeContains = "2"
want.Phones = []string{"+79501684430"} // from 8950...

// Case 8 (id=21)
want.Type = TypeLink

// Case 9 (id=12)
want.Type = TypeEmpty

// Case 10 (id=32)
want.Type = TypeLost
want.Phones = []string{"+79224052612"}
want.ContactNamesHas = []string{"–¢–∞—Ç—å—è–Ω–∞"}
want.LocationContains = []string{"–õ—é–∫—à—É–¥—å—è", "–®–∞–±–µ—Ä–¥–∏–Ω–æ"}

// Case 11 (id=29)
want.Type = TypeAdoption
want.ExtrasSterilized = true
want.ExtrasVaccinated = true
want.ExtrasLitterOK = true

// Case 12 (id=44)
want.Type = TypeAdoption
want.Animal = AnimalCat
want.Phones = []string{"+79068168418"} // from "8 906 816 84 18"
```

## Test utilities

Inside `domain_test.go`, add tiny helpers:

```go
func containsAll(hay string, needles []string) bool
func sliceHasAll(hay []string, needles []string) bool
func sliceHasAny(hay []string, needles []string) bool
func mustEqualSlices(t *testing.T, got, want []string)
```

Assert with `t.Fatalf/t.Errorf` only.

---

# Notes/constraints for Codex

* No `init()`, no I/O, no time, no randomness, no external packages.
* Use `regexp` and `strings`, `unicode`, `sort`.
* Keep all regex compiled once at package level:

  ```go
  var reLost = regexp.MustCompile(`(?i)\b(–ø—Ä–æ–ø–∞–ª[–∞–∏]?|–ø–æ—Ç–µ—Ä—è–ª[–∞—Å—å—Å—è]?|—É–±–µ–∂–∞–ª[–∞–∏]?|—Å–±–µ–∂–∞–ª[–∞–∏]?)\b`)
  // ...etc
  ```
* All helpers must be **pure**: input string ‚Üí output value, with no global mutations.
* Dedupe functions should preserve input order (use small map\[string]struct{} guard).
* Keep `extractLocationHeuristic` simple: split on `[,.;\n]`, filter segments with trigger tokens, choose shortest.

---

If you want, I can turn this plan into actual `domain.go` + `domain_test.go` scaffolding next ‚Äî but this spec is already ‚Äúcodex-ready‚Äù: drop it in and let the tool implement functions and tests to match.
